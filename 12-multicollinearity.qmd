---
title: "Multicollinearity"
subtitle: "Multiple linear regression -- Stat 230"
format:
  pdf:
    geometry:
      - margin=1in
---

```{r setup}
#| include: false
library(ggformula)
library(car)
library(broom)
library(dplyr)
library(GGally)
library(mosaic)

task <- 0
```

\vspace{-0.5in}

In this class, you will explore the issue of multicollinearity, when two or more explanatory variables are highly correlated. The multiple linear regression model does not assume that the explanatory variables are independent, so multicollinearity is not a violation of our model assumptions, but it does cause problems with interpretations and inference for our coefficients.

## Data overview

The owner of an apartment building in Minneapolis believed that their property tax bill was too high because of an over-assessment of the property's value by the city tax assessor. They hired an independent real estate appraiser to investigate the appropriateness of the city's assessment. In the original data set collected, there were five explanatory variables which the appraiser believed would be relevant to predicting sales price. However, when the appraiser reported their results to the owner of the apartment building, they noticed that the model did not take into account the level of the neighborhood amenities. As a good proxy for this rather hard-to-quantify feature, the appraiser decided to use the total payroll and and average salary in of residents in their analysis. Their argument was that the total payroll variable measures the level of commercial activity in the neighborhood, while the average salary variable is a measure of the affluence of neighborhood residents. Since these data are publicly available from the City of Minneapolis' Department of Community, Planning and Economic Development, the assessor records these data in the previous data set.

| Variable names | Description                      |
|----------------|----------------------------------|
| `price`        | Sale price                       |
| `units`        | Number of apartments             |
| `age`          | Age of structure (years)         |
| `lot`          | Lot Size (sq. ft)                |
| `parking`      | Number of On-Site Parking Spaces |
| `sqft`         | Gross Building Area (sq. ft)     |
| `payroll`      | Total Payroll                    |
| `salary`       | Average salary                   |

You can load this data set using the command

```{r}
mnsales <- read.csv("https://aloy.rbind.io/data/mnsales.csv")
```

<!-- ## What is this Adjusted R-squared that R keeps reporting? -->

<!-- Before we launch into our investigation of multicollinearity, let's discuss a common question that I have received about the summary output from a regression model in R: "what is this Adjusted R-squared value?" -->

<!--  Fit three regression models using `price` as the response variable. For each, report both the $R^2$ and the adjusted $R^2$ values. Are they different? How do they change? -->

<!-- :::{.callout-note} -->

<!-- The `glance()` function in the {broom} package extracts some summary statistics from a regression model, such as the  $R^2$ and the adjusted $R^2$ values, the F-statistic, and the estimated residual standard deviation. -->

<!-- ::: -->

<!-- (a) Use only `sqft` as a predictor variable. -->

<!-- (a) Use only `sqft` and `age` as predictor variables. -->

<!-- (a) Use only `sqft`, `age`, and `parking` as predictor variables. -->

<!-- Recall that $R^2$ describes the proportion of variability in the response that is explained by the regression model. Adding a new predictor variable will only increase the amount of variability explained, or at the worst have no impact (if the added predictor is independent of the response). Consequently, $R^2$ is not a useful metric to compare fitted models with different numbers of predictor variables.  -->

<!-- The adjusted $R^2$, $R^2_{\mathrm{adj}}$, imposes a *penalty for model complexity*, so it increases only if the improvement (i.e., increase in the explained variability) outweighs the cost of making the model more complex. The formula for $R^2$ is given by -->

<!-- $$ -->

<!-- R^2_{\mathrm{adj}} = 1 - \left( \frac{n-1}{n-p-1} \right)  \cdot \left(1 - R^2 \right) -->

<!-- $$ -->

<!-- Notice that the fraction $(n-1)/(n-p-1)$ is the *penalty term* and is close to 1 if the model is small (i.e., if $p-1$ is close to $1$). -->

<!-- So which version of $R^2$ should you report? If you are just looking at a single model or models all of the same size (i.e., same number of coefficients), then the "original" $R^2$ is fine. If you want to compare the fit of different size regression models, the use $R^2_{\mathrm{adj}}$. -->

**Task `r (task <- task + 1)`.** Fit a multiple regression model using all explanatory variables. Report the equation of the fitted model.

```{r}
#| include: false
full_mod <- lm(price ~ . , data = mnsales)
```

**Task `r (task <- task + 1)`.** Conduct an F-test of the overall utility of your multiple regression model (this is the `F-statistic` reported at the end of the `summary()` output). Make sure to state a null and an alternative hypothesis, the value of the test statistic, a p-value, and a conclusion within the context of the problem.

```{r}
#| include: false
#| eval: false
broom::glance(full_mod)
```

**Task `r (task <- task + 1)`.** In the daily prep, we discussed several indicators of multicollinearity. List them.

**Task `r (task <- task + 1)`.** Make the scatterplot matrix for all possible explanatory variables. Do any pairs seem highly correlated?

**Task `r (task <- task + 1)`.** Calculate the correlation matrix using the below code. Are any pairs highly correlated?

```{r}
#| include: false
#| eval: false
cor(mnsales, use = "complete.obs")
```

**Task `r (task <- task + 1)`.** Print the table of coefficients for your regression model. Do any of the estimated coefficient ***disagree*** with the pairwise correlations with weight? (That is, do any have different signs?)

**Task `r (task <- task + 1)`.** Looking again at the table of coefficients, which of the variables appear to be important predictors at the $\alpha = 0.05$ level? Do any of these results surprise you?

**Task `r (task <- task + 1)`.** Reflecting you your answers so far, are there any signs of multicollinearity?

**Task `r (task <- task + 1)`.** Another diagnostic tool used to detect multicollinearity is the variance inflation faction (VIF). To calculate the VIF values for each predictor, load the {car} package and run the following command (assuming that your model is called `mod1`). Are there any signs of multicollinearity based on the VIF values?

```{r}
#| eval: false
car::vif(full_mod)
```

**Task `r (task <- task + 1)`.** You should have found indications of multicollinearity in the previous task. Based on the stated goal of the analysis, should we be concerned about this? Why or why not?

**Task `r (task <- task + 1)` (If there's time)** Regardless of your answer to the previous task, let's try to remedy the situation with multicollinearity. Propose a reduced model and check whether it resolves the issue with multicollinearity.
